
# Parameters for each of the models (by jet number and first feature)

# Best results (always the recent configuration)

#Cross validation f1 score is 34.34 % and accuracy is 95.03 %
#Training F1 score is 35.71 % and accuracy is 95.13 %
#
#Cross validation f1 score is 68.68 % and accuracy is 81.17 %
#Training F1 score is 68.84 % and accuracy is 81.28 %
#
#Cross validation f1 score is 34.77 % and accuracy is 91.96 %
#Training F1 score is 40.77 % and accuracy is 92.70 %
#
#Cross validation f1 score is 73.55 % and accuracy is 79.85 %
#Training F1 score is 73.95 % and accuracy is 80.16 %
#
#Cross validation f1 score is 60.16 % and accuracy is 91.50 %
#Training F1 score is 67.46 % and accuracy is 93.23 %
#
#Cross validation f1 score is 82.80 % and accuracy is 83.66 %
#Training F1 score is 83.23 % and accuracy is 84.09 %
#
#Overall validation F1 score is 59.05 % and accuracy is 87.20 %

zero_jet_0:
  # 26123 samples

  # Model type
  model: ridge
  # step_size
  gamma: null
  # number of steps to run
  max_iters: null
  # regularization parameter 1e-8
  lambda: 0.00000001
  # ratio split
  ratio: null
  # Enable cross validation
  cross_val: True
  # Number of folds for cross-validation
  kfold: 5
  # Whether to build polynomial features or not
  build_poly: True
  # Degrees for polynomial features
  degree: 10
  # Wheather to multiply each column with each column and expand features
  multiply_each: True
  # Take square root of features
  square_root: True
  # Apply log transform as scaler
  log_transform: True
  # Replace -999 values with specified value, this can take 'zero', 'mean', 'median', 'mode' or null
  replace_with: 'median'
  # Apply only normalization
  only_normalize: False
  # Logistic regression threshold
  reg_threshold: 0.5
  # remove outliers:
  remove_outliers: False
  # Drop correlated features
  drop_corr: False
  # Expand angle features by taking cos and sin
  trig: False

  # Paths
  train_data: "../data/train.csv"
  test_data: "../data/test.csv"
  # Submissions output_path
  output_path: "../results/"
  # Visualizations output path
  viz_path: "../visualizations/"

zero_jet_1:
  # 73790 samples
  # Model type
  model: ridge
  # step_size
  gamma: null
  # number of steps to run
  max_iters: null
  # regularization parameter 1e-14
  lambda: 0.00000000000001
  # ratio split
  ratio: null
  # Enable cross validation
  cross_val: True
  # Number of folds for cross-validation
  kfold: 5
  # Whether to build polynomial features or not
  build_poly: True
  # Degrees for polynomial features
  degree: 10
  # Wheather to multiply each column with each column and expand features
  multiply_each: True
  # Take square root of features
  square_root: True
  # Apply log transform as scaler
  log_transform: False
  # Replace -999 values with specified value, this can take 'zero', 'mean', 'median', 'mode' or null
  replace_with: 'median'
  # Apply only normalization
  only_normalize: False
  # Logistic regression threshold
  reg_threshold: 0.5
  # remove outliers:
  remove_outliers: False
  # Drop correlated features
  drop_corr: False
  # Expand angle features by taking cos and sin
  trig: False

  # Paths
  train_data: "../data/train.csv"
  test_data: "../data/test.csv"
  # Submissions output_path
  output_path: "../results/"
  # Visualizations output path
  viz_path: "../visualizations/"


one_jet_0:
  # 7562 samples

  # Model type
  model: ridge
  # step_size
  gamma: null
  # number of steps to run
  max_iters: null
  # regularization parameter 1e-8
  lambda: 0.00000001
  # ratio split
  ratio: null
  # Enable cross validation
  cross_val: True
  # Number of folds for cross-validation
  kfold: 5
  # Whether to build polynomial features or not
  build_poly: True
  # Degrees for polynomial features
  degree: 6
  # Wheather to multiply each column with each column and expand features
  multiply_each: True
  # Take square root of features
  square_root: True
  # Apply log transform as scaler
  log_transform: False
  # Replace -999 values with specified value, this can take 'zero', 'mean', 'median', 'mode' or null
  replace_with: 'mode'
  # Apply only normalization
  only_normalize: False
  # Logistic regression threshold
  reg_threshold: 0.5
  # remove outliers:
  remove_outliers: False
  # Drop correlated features
  drop_corr: False
  # Expand angle features by taking cos and sin
  trig: False

  # Paths
  train_data: "../data/train.csv"
  test_data: "../data/test.csv"
  # Submissions output_path
  output_path: "../results/"
  # Visualizations output path
  viz_path: "../visualizations/"

one_jet_1:
  # 69982 samples
  # Model type
  model: ridge
  # step_size
  gamma: null
  # number of steps to run
  max_iters: null
  # regularization parameter 1e-16
  lambda: 0.0000000000000001
  # ratio split
  ratio: null
  # Enable cross validation
  cross_val: True
  # Number of folds for cross-validation
  kfold: 5
  # Whether to build polynomial features or not
  build_poly: True
  # Degrees for polynomial features
  degree: 10
  # Wheather to multiply each column with each column and expand features
  multiply_each: True
  # Take square root of features
  square_root: True
  # Apply log transform as scaler
  log_transform: False
  # Replace -999 values with specified value, this can take 'zero', 'mean', 'median', 'mode' or null
  replace_with: 'mode'
  # Apply only normalization
  only_normalize: False
  # Logistic regression threshold
  reg_threshold: 0.5
  # remove outliers:
  remove_outliers: False
  # Drop correlated features
  drop_corr: False
  # Expand angle features by taking cos and sin
  trig: False

  # Paths
  train_data: "../data/train.csv"
  test_data: "../data/test.csv"
  # Submissions output_path
  output_path: "../results/"
  # Visualizations output path
  viz_path: "../visualizations/"

more_than_one_jet_0:
  # 4429 samples
  # Model type
  model: ridge
  # step_size
  gamma: null
  # number of steps to run
  max_iters: null
  # regularization parameter 1e-3
  lambda: 0.001
  # ratio split
  ratio: null
  # Enable cross validation
  cross_val: True
  # Number of folds for cross-validation
  kfold: 5
  # Whether to build polynomial features or not
  build_poly: True
  # Degrees for polynomial features
  degree: 3
  # Wheather to multiply each column with each column and expand features
  multiply_each: True
  # Take square root of features
  square_root: True
  # Apply log transform as scaler
  log_transform: False
  # Replace -999 values with specified value, this can take 'zero', 'mean', 'median', 'mode' or null
  replace_with: 'mode'
  # Apply only normalization
  only_normalize: False
  # Logistic regression threshold
  reg_threshold: 0.5
  # remove outliers:
  remove_outliers: False
  # Drop correlated features
  drop_corr: False
  # Expand angle features by taking cos and sin
  trig: False

  # Paths
  train_data: "../data/train.csv"
  test_data: "../data/test.csv"
  # Submissions output_path
  output_path: "../results/"
  # Visualizations output path
  viz_path: "../visualizations/"

more_than_one_jet_1:
  # 68114 samples
  # Model type
  model: ridge
  # step_size
  gamma: null
  # number of steps to run
  max_iters: null
  # regularization parameter 1e-15
  lambda: 0.000000000000001
  # ratio split
  ratio: null
  # Enable cross validation
  cross_val: True
  # Number of folds for cross-validation
  kfold: 5
  # Whether to build polynomial features or not
  build_poly: True
  # Degrees for polynomial features
  degree: 11
  # Wheather to multiply each column with each column and expand features
  multiply_each: True
  # Take square root of features
  square_root: True
  # Apply log transform as scaler
  log_transform: False
  # Replace -999 values with specified value, this can take 'zero', 'mean', 'median', 'mode' or null
  replace_with: 'mode'
  # Apply only normalization
  only_normalize: False
  # Logistic regression threshold
  reg_threshold: 0.5
  # remove outliers:
  remove_outliers: False
  # Drop correlated features
  drop_corr: False
  # Expand angle features by taking cos and sin
  trig: False

  # Paths
  train_data: "../data/train.csv"
  test_data: "../data/test.csv"
  # Submissions output_path
  output_path: "../results/"
  # Visualizations output path
  viz_path: "../visualizations/"

# Paths
train_data: "../data/train.csv"
test_data: "../data/test.csv"
# Submissions output_path
output_path: "../results/"
# Visualizations output path
viz_path: "../visualizations/"