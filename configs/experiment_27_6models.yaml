# Parameters for each of the models (by jet number and first feature)

#Cross validation f1 score is 14.70 % and accuracy is 95.61 %
#Training F1 score is 16.76 % and accuracy is 95.72 %
#
#Cross validation f1 score is 67.60 % and accuracy is 79.89 %
#Training F1 score is 67.75 % and accuracy is 79.99 %
#
#Cross validation f1 score is 34.90 % and accuracy is 91.75 %
#Training F1 score is 41.67 % and accuracy is 92.78 %
#
#Cross validation f1 score is 73.61 % and accuracy is 79.88 %
#Training F1 score is 73.95 % and accuracy is 80.16 %
#
#Cross validation f1 score is 55.60 % and accuracy is 90.17 %
#Training F1 score is 71.50 % and accuracy is 93.88 %
#
#Cross validation f1 score is 82.65 % and accuracy is 83.54 %
#Training F1 score is 83.21 % and accuracy is 84.08 %
#
#Overall validation F1 score is 54.84 % and accuracy is 86.81 %

zero_jet_0:
  # 26123 samples

#  Cross validation f1 score is 34.73 % and accuracy is 95.08 %
#  Training F1 score is 35.40 % and accuracy is 95.14 %

  # Model type
  model: ridge
  # step_size
  gamma: null
  # number of steps to run
  max_iters: null
  # regularization parameter 1e-14
  lambda: 0.00000001
  # ratio split
  ratio: null
  # Enable cross validation
  cross_val: True
  # Number of folds for cross-validation
  kfold: 5
  # Whether to build polynomial features or not
  build_poly: True
  # Degrees for polynomial features
  degree: 9
  # Wheather to multiply each column with each column and expand features
  multiply_each: True
  # Take square root of features
  square_root: True
  # Apply log transform as scaler
  log_transform: True
  # Replace -999 values with specified value, this can take 'zero', 'mean', 'median', 'mode' or null
  replace_with: 'median'
  # Apply only normalization
  only_normalize: False
  # Logistic regression threshold
  reg_threshold: 0.5
  # remove outliers:
  remove_outliers: False
  # Drop correlated features
  drop_corr: False
  # Expand angle features by taking cos and sin
  trig: False

  # Paths
  train_data: "../data/train.csv"
  test_data: "../data/test.csv"
  # Submissions output_path
  output_path: "../results/"
  # Visualizations output path
  viz_path: "../visualizations/"

zero_jet_1:
  # 73790 samples
  # Model type
  model: ridge
  # step_size
  gamma: null
  # number of steps to run
  max_iters: null
  # regularization parameter 1e-14
  lambda: 0.00000000000001
  # ratio split
  ratio: null
  # Enable cross validation
  cross_val: True
  # Number of folds for cross-validation
  kfold: 5
  # Whether to build polynomial features or not
  build_poly: True
  # Degrees for polynomial features
  degree: 12
  # Wheather to multiply each column with each column and expand features
  multiply_each: True
  # Take square root of features
  square_root: True
  # Apply log transform as scaler
  log_transform: False
  # Replace -999 values with specified value, this can take 'zero', 'mean', 'median', 'mode' or null
  replace_with: 'median'
  # Apply only normalization
  only_normalize: False
  # Logistic regression threshold
  reg_threshold: 0.5
  # remove outliers:
  remove_outliers: False
  # Drop correlated features
  drop_corr: False
  # Expand angle features by taking cos and sin
  trig: False

  # Paths
  train_data: "../data/train.csv"
  test_data: "../data/test.csv"
  # Submissions output_path
  output_path: "../results/"
  # Visualizations output path
  viz_path: "../visualizations/"


one_jet_0:
  # 7562 samples
  # 10
  #Cross validation f1 score is 34.90 % and accuracy is 91.75 %
  #Training F1 score is 41.67 % and accuracy is 92.78 %
  # 11
  # Cross validation f1 score is 34.69 % and accuracy is 91.64 %
  # Training F1 score is 41.84 % and accuracy is 92.79 %
  # 9
#  Cross validation f1 score is 35.35 % and accuracy is 91.71 %
#  Training F1 score is 41.28 % and accuracy is 92.74 %
  # 9 -12
#  Cross validation f1 score is 35.61 % and accuracy is 91.92 %
#  Training F1 score is 41.03 % and accuracy is 92.70 %
# 10 -12 log transform
#Cross validation f1 score is 34.87 % and accuracy is 91.79 %
#Training F1 score is 40.94 % and accuracy is 92.71 %

  # Model type
  model: ridge
  # step_size
  gamma: null
  # number of steps to run
  max_iters: null
  # regularization parameter 1e-16
  lambda: 0.000000000001
  # ratio split
  ratio: null
  # Enable cross validation
  cross_val: True
  # Number of folds for cross-validation
  kfold: 5
  # Whether to build polynomial features or not
  build_poly: True
  # Degrees for polynomial features
  degree: 10
  # Wheather to multiply each column with each column and expand features
  multiply_each: True
  # Take square root of features
  square_root: True
  # Apply log transform as scaler
  log_transform: False
  # Replace -999 values with specified value, this can take 'zero', 'mean', 'median', 'mode' or null
  replace_with: 'mode'
  # Apply only normalization
  only_normalize: False
  # Logistic regression threshold
  reg_threshold: 0.5
  # remove outliers:
  remove_outliers: False
  # Drop correlated features
  drop_corr: False
  # Expand angle features by taking cos and sin
  trig: False

  # Paths
  train_data: "../data/train.csv"
  test_data: "../data/test.csv"
  # Submissions output_path
  output_path: "../results/"
  # Visualizations output path
  viz_path: "../visualizations/"

one_jet_1:
  # 69982 samples
  # Model type
  model: ridge
  # step_size
  gamma: null
  # number of steps to run
  max_iters: null
  # regularization parameter 1e-16
  lambda: 0.0000000000000001
  # ratio split
  ratio: null
  # Enable cross validation
  cross_val: True
  # Number of folds for cross-validation
  kfold: 5
  # Whether to build polynomial features or not
  build_poly: True
  # Degrees for polynomial features
  degree: 10
  # Wheather to multiply each column with each column and expand features
  multiply_each: True
  # Take square root of features
  square_root: True
  # Apply log transform as scaler
  log_transform: False
  # Replace -999 values with specified value, this can take 'zero', 'mean', 'median', 'mode' or null
  replace_with: 'mode'
  # Apply only normalization
  only_normalize: False
  # Logistic regression threshold
  reg_threshold: 0.5
  # remove outliers:
  remove_outliers: False
  # Drop correlated features
  drop_corr: False
  # Expand angle features by taking cos and sin
  trig: False

  # Paths
  train_data: "../data/train.csv"
  test_data: "../data/test.csv"
  # Submissions output_path
  output_path: "../results/"
  # Visualizations output path
  viz_path: "../visualizations/"

more_than_one_jet_0:
  # 4429 samples
  # Model type
  model: ridge
  # step_size
  gamma: null
  # number of steps to run
  max_iters: null
  # regularization parameter 1e-14
  lambda: 0.00000000000001
  # ratio split
  ratio: null
  # Enable cross validation
  cross_val: True
  # Number of folds for cross-validation
  kfold: 5
  # Whether to build polynomial features or not
  build_poly: True
  # Degrees for polynomial features
  degree: 11
  # Wheather to multiply each column with each column and expand features
  multiply_each: True
  # Take square root of features
  square_root: True
  # Apply log transform as scaler
  log_transform: False
  # Replace -999 values with specified value, this can take 'zero', 'mean', 'median', 'mode' or null
  replace_with: 'mode'
  # Apply only normalization
  only_normalize: False
  # Logistic regression threshold
  reg_threshold: 0.5
  # remove outliers:
  remove_outliers: False
  # Drop correlated features
  drop_corr: False
  # Expand angle features by taking cos and sin
  trig: False

  # Paths
  train_data: "../data/train.csv"
  test_data: "../data/test.csv"
  # Submissions output_path
  output_path: "../results/"
  # Visualizations output path
  viz_path: "../visualizations/"

more_than_one_jet_1:
  # 68114 samples
  # Model type
  model: ridge
  # step_size
  gamma: null
  # number of steps to run
  max_iters: null
  # regularization parameter 1e-14
  lambda: 0.00000000000001
  # ratio split
  ratio: null
  # Enable cross validation
  cross_val: True
  # Number of folds for cross-validation
  kfold: 5
  # Whether to build polynomial features or not
  build_poly: True
  # Degrees for polynomial features
  degree: 11
  # Wheather to multiply each column with each column and expand features
  multiply_each: True
  # Take square root of features
  square_root: True
  # Apply log transform as scaler
  log_transform: False
  # Replace -999 values with specified value, this can take 'zero', 'mean', 'median', 'mode' or null
  replace_with: 'mode'
  # Apply only normalization
  only_normalize: False
  # Logistic regression threshold
  reg_threshold: 0.5
  # remove outliers:
  remove_outliers: False
  # Drop correlated features
  drop_corr: False
  # Expand angle features by taking cos and sin
  trig: False

  # Paths
  train_data: "../data/train.csv"
  test_data: "../data/test.csv"
  # Submissions output_path
  output_path: "../results/"
  # Visualizations output path
  viz_path: "../visualizations/"

# Paths
train_data: "../data/train.csv"
test_data: "../data/test.csv"
# Submissions output_path
output_path: "../results/"
# Visualizations output path
viz_path: "../visualizations/"