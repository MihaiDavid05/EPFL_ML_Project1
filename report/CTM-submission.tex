\documentclass[10pt,conference,compsocconf]{IEEEtran}

\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{booktabs}


\begin{document}
\title{Writing Scientific Papers and Software}

\author{
  Capucine Berger-Sigrist, Mihai David, Tiberiu Mosnoi\\
  \textit{Department of Computer Science, EPFL, Switzerland}
}

\maketitle
%%*************************************************************************
%%*************************************************************************
\begin{abstract}
A critical part of scientific discovery is the
communication of research findings to peers or the general public.
Mastery of the process of scientific communication improves the
visibility and impact of research. While this guide is a necessary
tool for learning how to write in a manner suitable for publication
at a scientific venue, it is by no means sufficient, on its own, to
make its reader an accomplished writer. 
This guide should be a starting point for further development of 
writing skills.
Short description of the whole paper, to help the
reader decide whether to read it. ~\cite{higgsMLdoc}
\begin{enumerate}
    \item State the problem.
    \item Say why it is an interesting problem.
    \item Say what your solution achieves.
    \item Say what follows from your solution.
\end{enumerate}
\end{abstract}

%%*************************************************************************
%%*************************************************************************
\section{Introduction}
%%Describe your problem and state your
%%contributions. The hope is that after reading your
%%paper, the audience will be convinced to try out your idea.

%%At that point, it is important that the reader is able to reproduce your
%%work~\cite{schwab00,wavelab,gentleman05}. This is why it is also
%%important that if the work has a computational component, the software
%%associated with producing the results are also made available in a
%%useful form. Several guidelines for making your user's experience with
%%your software as painless as possible is given in
%%Section~\ref{sec:tips-software}.

The ATLAS and CMS experiments were able to confirm the existence of 
the Higgs boson by analysing the decay of particles produced by 
the collision of protons in the CERN's LHC. 
As such, using a combination of regression and data preprocessing 
methods, the aim of this project is to predict whether the observed 
signals come from the decay of a Higgs boson or from background noise.

%%*************************************************************************
%%*************************************************************************
\section{Models and Methods}
%%The models and methods section should describe what was
%%done to answer the research question, describe how it was done,
%%justify the experimental design, and explain how the results were analyzed.

%%The model refers to the underlying mathematical model or structure which 
%%you use to describe your problem, or that your solution is based on. 
%%The methods on the other hand, are the algorithms used to solve the problem. 
%%In some cases, the suggested method directly solves the problem, without having it 
%%stated in terms of an underlying model. Generally though it is a better practice to have 
%%the model figured out and stated clearly, rather than presenting a method without specifying 
%%the model. In this case, the method can be more easily evaluated in the task of fitting 
%%the given data to the underlying model.

%%The methods part of this section, detailed enough such
%%that an interested reader can reproduce your
%%work~\cite{anderson04,wavelab}.

%%clear and precise description of how an experiment was done, and the rationale
%%for why specific experimental procedures were chosen. It is usually helpful to
%%structure the methods section by~\cite{kallet04methods}:
%%\begin{enumerate}
%%\item Layout the model you used to describe the problem or the solution.
%%\item Describing the algorithms used in the study, briefly including
%%  details such as hyperparameter values (e.g. thresholds), and
%%  preprocessing steps (e.g. normalizing the data to have mean value of
%%  zero).
%%\item Explaining how the materials were prepared, for example the
%%  images used and their resolution.
%%\item Describing the research protocol, for example which examples
%%  were used for estimating the parameters (training) and which were
%%  used for computing performance.
%%\item Explaining how measurements were made and what
%%  calculations were performed. Do not reproduce the full source code in
%%  the paper, but explain the key steps.
%%\end{enumerate}

We used a linear regression model on the train set to effectively 
predict the outputs for the data of the test set. For this we implemented 
different methods, one of them being least squares and optimized the 
loss using gradient or stochastic gradient descent.
Our second method relied on ridge regression to penalise large weights 
vectors and avoid potentially over or underfitting the model.
Finally the last solution we explored was logistic and regularized 
logistic regression. During this project we chose to focus our attention on
the last two methods.

\begin{table}[ht]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}lrrrrrrr@{}}
\toprule
                                                                           & \multicolumn{5}{c}{Parameters} & \multicolumn{2}{c}{Training} \\ \midrule
Methods &
  \multicolumn{1}{l}{Gamma} &
  \multicolumn{1}{l}{Lambda} &
  \multicolumn{1}{l}{Degree} &
  \multicolumn{1}{l}{Max\_iters} &
  \multicolumn{1}{l}{Reg\_thres} &
  \multicolumn{1}{l}{F1-score} &
  \multicolumn{1}{l}{Accuracy} \\ \midrule
Logistic regression                                                        & 0.5  & -      & 1 & 1000 & 0.5 & 44.73\%       & 72.78\%      \\
\begin{tabular}[c]{@{}l@{}}Regularized logistic \\ regression\end{tabular} & 0.4  & 0.05   & 1 & 1000 & 0.5 & 44.63\%       & 72.76\%      \\
Ridge Regression                                                           & -    & 0.0001 & 2 & -    & 0.5 & 61.47\%       & 76.19\%      \\ \bottomrule
\end{tabular}%
}
\caption{Parameters and training results for the initial implementation.}
\label{tab:baseline}
\end{table}

Table~\ref{tab:baseline} shows the initial results obtained and act as a baseline 
on which we could compare our different configurations.

Analysis of first results.

\subsection{Hyperparameters}
To tune the hyperparameters for regularized logistic regression, 
we first devised a grid search method to compute \textit{(lambda, degree)} 
pairs using cross-validation and keeping the configuration 
that maximised the F1-score. With this first result we tried to obtain 
the optimal regularization threshold.
Unfortunately, this proved too computationally expensive. Therefore, 
we had to go for a trial and error methodology to pinpoint the most 
accurately the optimal parameters.
Still, even by tuning the parameters to the best of our abilities, 
we realised that to improve on our results we needed to analyse more 
careful the data-set.

\subsection{Data preprocessing}
The data set comes from the simulation of 250.000 simulated events 
of proton-proton collisions. Each data is composed of 30 features 
that can be divided into two categories: primitive features related 
to the momenta of particles or derived feature obtained from the former.

\subsubsection{Feature distribution}
Our first step in the data preprocessing was to standardize the features 
to ensure internal consistency. Data points were considered to be 
outliers if they were at +/- 3 standard deviation from their mean and 
were, as such, removed. We later observed that most of the features 
were right skewed. As such, using the log transform should have enabled 
us to have a more balanced distribution, but after multiple experiments, 
we realised that it did not improve our predictions. 
Finally, undefined features represented by the value -999.0 or Nan, 
were replaced either by their mean, median or mode. Overall, the 
latter two tended to yield a better accuracy and F1-score.

\subsubsection{Feature expansion}
By augmenting the feature vectors we found that we could obtain a more 
powerful model. One way was to do that was to use polynomial expansion 
by carefully choosing a degree d:
$$\phi := [1, X, X^2, ..., X^d]$$
where $X$ represents the features matrix and then fitting our model 
to the new matrix $\phi$. Atop of this, we increased the complexity 
of the model by multiplying each feature together:
$$\forall \phi_{:i} \neq \phi_{:j}, \;\; \phi := [\phi\; |\; \phi_{:i} * \phi_{:j}] $$
And then we applied the square root to the positive feature columns 
of $\phi$.

\subsubsection{Intermediate results}
cross val k = 5
Log: square root, replace with median, remove outliers 'std'
reg log: square root, replace with median, remove outliers 'std'
ridge: square root, replace with mode, don't remove outliers


\begin{table}[ht]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}lrrrrrrr@{}}
\toprule
                                                                           & \multicolumn{5}{c}{Parameters}   & \multicolumn{2}{c}{Training} \\ \midrule
Methods &
  \multicolumn{1}{l}{Gamma} &
  \multicolumn{1}{l}{Lambda} &
  \multicolumn{1}{l}{Degree} &
  \multicolumn{1}{l}{Max\_iters} &
  \multicolumn{1}{l}{Reg\_thres} &
  \multicolumn{1}{l}{F1-score} &
  \multicolumn{1}{l}{Accuracy} \\ \midrule
Logistic regression                                                        & 0.5 & -      & 3  & 10000 & 0.05 & 71.2\%        & 80.1\%       \\
\begin{tabular}[c]{@{}l@{}}Regularized logistic \\ regression\end{tabular} & 0.5 & 0.05   & 6  & 10000 & 0.05 & 67.9\%        & 79.4\%       \\
Ridge Regression                                                           & -   & 0.0001 & 15 & -     & 0.5  & 72.9\%        & 82.2\%       \\ \bottomrule
\end{tabular}%
}
\caption{Parameters and training results after preprocessing the data.}
\label{tab:entire_dataset}
\end{table}

We expected that logistic regression would fare better, seeing as it uses 
the sigmoid function and is, therefore, less liable to be influenced by extreme 
range of values, as is the case in the data-set. 
But as Table~\ref{tab:entire_dataset} shows, ridge regression has a (slightly ?) better 
accuracy and F1-score.

\subsubsection{Further data analysis}
The description of the feature in the Learning to discover: the Higgs
boson machine learning challenge~\cite{higgsMLdoc} documentation pointed 
to the fact that a lot of features depend on the value (either 0, 1, 2 or 3)
of the \textit{PRI\_jet\_num} feature.

Inspecting the data, we saw that some features (up to eleven out of the thirty) 
were marked as undefined for data points with a \textit{PRI\_jet\_num} of 0 or 1, 
which is not the case for 2 or 3. Consequently, we decided to split the 
original data-set into three subsets according to their \textit{PRI\_jet\_num} value: 
\begin{enumerate}
    \item \textit{PRI\_jet\_0} with N = 99913
    \item \textit{PRI\_jet\_1} with N=77544
    \item \textit{PRI\_jet\_2\_3} with N=72543
\end{enumerate}
\textit{PRI\_jet\_num} 2 and 3 where grouped together to have more 
balanced subsets.

%%*************************************************************************
%%*************************************************************************
\section{Results}
From Table~\ref{tab:entire_dataset} we saw that, overall, ridge regression
performed better and had the advantage of being computationally faster.
As such, we chose to focus our attention on this model. For each subset we
tuned the parameters accordingly and preprocessed the feature matrix 
following the aforementioned methods. 
We used a 10-fold cross-validation to estimate the generalized error or the 
model and its variance.

\begin{table}[ht]
\centering
\begin{tabular}{@{}lrrrr@{}}
\toprule
Ridge regression & \multicolumn{2}{c}{Parameters}              & \multicolumn{2}{c}{Cross-validation} \\ \midrule
Subsets          & Lambda               & Degree               & F1-score          & Accuracy         \\ \midrule
PRI\_jet\_0      & 1e-14                & 12                   & 66.53 \%          & 84.59 \%         \\
PRI\_jet\_1      & 1e-16                & 10                   & 72.87 \%          & 80.98 \%         \\
PRI\_jet\_2\_3   & 1e-16                & 11                   & 82.11 \%          & 83.82 \%         \\ \midrule
Overall          & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & 73.84\%           & 83.13\%          \\ \bottomrule
\end{tabular}
\caption{Parameters and cross-validation results for each of the subset and the overall 
F1-score and accuracy.}
\label{tab:cv_results}
\end{table}

In contrast to subset 1 and 2 and 3, we saw that subset 0 yielded better 
result if these values were replaced by the median rather than the mode.

subset 0: multiply each, and square root, median, don't remove outliers
subset 1: multiply each, and square root, mode, don't remove outliers
subset 2: multiply each, and square root, mode, don't remove outliers

%%When reporting computational or measurement results, always
%%report the mean (average value) along with a measure of variability
%%(standard deviation(s) or standard error of the mean).
%%*************************************************************************
%%*************************************************************************
\section{Discussion}
%%Discuss the strengths and weaknesses of your approach, 
%%based on the results. Point out the implications of your novel idea on the application concerned.

%% Discussion ideas
Test results from AIcrowd accuracy 0.833 f1-score 0.749	
And try to understand why log transform does not help. tested trigonometric 
for values with angles in radian but did not help, neither did dropping correlated features.

would be great to be able to get exactly the best hyperparameters. 

Since in the optimal configuration we found that all subsets still had undefined values (-999) for the first feature \textit{DER\_mass\_MMC} an idea would be to split again each subset
depending on whether or not the feature is defined.

%%*************************************************************************
%%*************************************************************************
\section{Summary}
%%Summarize your contributions in light of the new results.

%%The aim of a scientific paper is to convey the idea or discovery of
%%the researcher to the minds of the readers. The associated software
%%%%package provides the relevant details, which are often only briefly
%%explained in the paper, such that the research can be reproduced.
%%To write good papers, identify your key idea, make your contributions
%%explicit, and use examples and illustrations to describe the problems
%%and solutions.
Data pre-processing was an integral part. 
%%*************************************************************************
%%*************************************************************************

\section*{Acknowledgements}
The author thanks Christian Sigg for his careful reading and helpful
suggestions.

%%*************************************************************************
%%*************************************************************************
\pagebreak
\section{The Structure of a Paper}
\label{sec:structure-paper}
Scientific papers usually begin with the description of the problem,
justifying why the problem is interesting. Most importantly, it argues
that the problem is still unsolved, or that the current solutions are
unsatisfactory. This leads to the main gist of the paper, which is
``the idea''. The authors then show evidence, using derivations or
experiments, that the idea works. Since science does not occur in a
vacuum, a proper comparison to the current state of the art is often
part of the results.

%%*************************************************************************
%%*************************************************************************

\section{Tips for Good Software}
\label{sec:tips-software}

\begin{itemize}
\item Have a \texttt{README} file that (at least) describes what your
  software does, and which commands to run to obtain results. Also
  mention anything special that needs to be set up, such as
  toolboxes.
\item A list of authors and contributors can be included in a file
  called \texttt{AUTHORS}, acknowledging any help that you may have
  obtained. For small projects, this information is often also
  included in the \texttt{README}.
\item Document your code. Each file should at least have a short
  description about its reason for existence. Non obvious steps in the
  code should be commented. Functions arguments and return values should be described.
\item Describe how the results presented in your paper can be reproduced.
\end{itemize}

\bibliographystyle{IEEEtran}
\bibliography{CTM-literature}

\end{document}
